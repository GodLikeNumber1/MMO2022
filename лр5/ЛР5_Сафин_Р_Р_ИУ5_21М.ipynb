{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ЛР5 Сафин Р.Р. ИУ5-21М.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xZpoo218Y2Ay"
      },
      "outputs": [],
      "source": [
        "text = '''Повседневная практика показывает, что начало повседневной работы по формированию позиции в значительной степени обуславливает создание модели развития? Таким образом, курс на социально-ориентированный национальный проект обеспечивает актуальность всесторонне сбалансированных нововведений. Дорогие друзья, реализация намеченного плана развития влечет за собой процесс внедрения и модернизации существующих финансовых и административных условий.'''\n",
        "text2 = 'Разнообразный и богатый опыт реализация намеченного плана развития требует от нас системного анализа дальнейших направлений развитая системы массового участия.'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PMfSMHPsiqP",
        "outputId": "21bc769b-725a-440e-d6db-203df379883c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задача токенизации"
      ],
      "metadata": {
        "id": "6D29cTbU76Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np693nsfbKDv",
        "outputId": "c567a905-cae4-452f-a116-e42088aabf09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer',\n",
              " '__builtins__',\n",
              " '__cached__']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tk_1 = nltk.WordPunctTokenizer()\n",
        "nltk_tk_1.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ5RiWNraFGc",
        "outputId": "5821dbbf-83e4-455a-c95a-a17fadc14d56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Повседневная',\n",
              " 'практика',\n",
              " 'показывает',\n",
              " ',',\n",
              " 'что',\n",
              " 'начало',\n",
              " 'повседневной',\n",
              " 'работы',\n",
              " 'по',\n",
              " 'формированию',\n",
              " 'позиции',\n",
              " 'в',\n",
              " 'значительной',\n",
              " 'степени',\n",
              " 'обуславливает',\n",
              " 'создание',\n",
              " 'модели',\n",
              " 'развития',\n",
              " '?',\n",
              " 'Таким',\n",
              " 'образом',\n",
              " ',',\n",
              " 'курс',\n",
              " 'на',\n",
              " 'социально',\n",
              " '-',\n",
              " 'ориентированный',\n",
              " 'национальный',\n",
              " 'проект',\n",
              " 'обеспечивает',\n",
              " 'актуальность',\n",
              " 'всесторонне',\n",
              " 'сбалансированных',\n",
              " 'нововведений',\n",
              " '.',\n",
              " 'Дорогие',\n",
              " 'друзья',\n",
              " ',',\n",
              " 'реализация',\n",
              " 'намеченного',\n",
              " 'плана',\n",
              " 'развития',\n",
              " 'влечет',\n",
              " 'за',\n",
              " 'собой',\n",
              " 'процесс',\n",
              " 'внедрения',\n",
              " 'и',\n",
              " 'модернизации',\n",
              " 'существующих',\n",
              " 'финансовых',\n",
              " 'и',\n",
              " 'административных',\n",
              " 'условий',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tk_sents = nltk.tokenize.sent_tokenize(text2)\n",
        "print(len(nltk_tk_sents))\n",
        "nltk_tk_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3qQdSq5aQqO",
        "outputId": "0ae34a4d-4996-4ffa-b8f2-37ae8ac3d6d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Разнообразный и богатый опыт реализация намеченного плана развития требует от нас системного анализа дальнейших направлений развитая системы массового участия.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Частеречная разметка"
      ],
      "metadata": {
        "id": "aWPyU6o68IxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1-zcPkHCvVK",
        "outputId": "e1cb9776-2aee-48c5-9c95-70e0323e22d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 151 kB/s \n",
            "\u001b[?25hCollecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 104 kB/s \n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 24.0 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=0747dfb845c725caf97a1a75990d2b451c0f207aeb1cb887e6d8843eb2d19beb\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, razdel, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from razdel import tokenize, sentenize"
      ],
      "metadata": {
        "id": "LFzt-a0LCt79"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "from slovnet import Morph"
      ],
      "metadata": {
        "id": "OJ5AC_rBj7Au"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Файл необходимо скачать по ссылке https://github.com/natasha/navec#downloads\n",
        "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')"
      ],
      "metadata": {
        "id": "GnUNaqiGj9x4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Файл необходимо скачать по ссылке https://github.com/natasha/slovnet#downloads\n",
        "n_morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)"
      ],
      "metadata": {
        "id": "JIbJX44Ck5TR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph_res = n_morph.navec(navec)"
      ],
      "metadata": {
        "id": "VGmz2S2OlB-x"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pos(markup):\n",
        "    for token in markup.tokens:\n",
        "        print('{} - {}'.format(token.text, token.tag))"
      ],
      "metadata": {
        "id": "PHeplM8slDbu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_sentenize(text):\n",
        "    n_sen_chunk = []\n",
        "    for sent in sentenize(text):\n",
        "        tokens = [_.text for _ in tokenize(sent.text)]\n",
        "        n_sen_chunk.append(tokens)\n",
        "    return n_sen_chunk"
      ],
      "metadata": {
        "id": "42QhiTrNJXup"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_sen_chunk = n_sentenize(text)\n",
        "n_sen_chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPrJjrAsJagv",
        "outputId": "87d27ce4-d347-4780-c6e2-774198f68c07"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Повседневная',\n",
              "  'практика',\n",
              "  'показывает',\n",
              "  ',',\n",
              "  'что',\n",
              "  'начало',\n",
              "  'повседневной',\n",
              "  'работы',\n",
              "  'по',\n",
              "  'формированию',\n",
              "  'позиции',\n",
              "  'в',\n",
              "  'значительной',\n",
              "  'степени',\n",
              "  'обуславливает',\n",
              "  'создание',\n",
              "  'модели',\n",
              "  'развития',\n",
              "  '?'],\n",
              " ['Таким',\n",
              "  'образом',\n",
              "  ',',\n",
              "  'курс',\n",
              "  'на',\n",
              "  'социально-ориентированный',\n",
              "  'национальный',\n",
              "  'проект',\n",
              "  'обеспечивает',\n",
              "  'актуальность',\n",
              "  'всесторонне',\n",
              "  'сбалансированных',\n",
              "  'нововведений',\n",
              "  '.'],\n",
              " ['Дорогие',\n",
              "  'друзья',\n",
              "  ',',\n",
              "  'реализация',\n",
              "  'намеченного',\n",
              "  'плана',\n",
              "  'развития',\n",
              "  'влечет',\n",
              "  'за',\n",
              "  'собой',\n",
              "  'процесс',\n",
              "  'внедрения',\n",
              "  'и',\n",
              "  'модернизации',\n",
              "  'существующих',\n",
              "  'финансовых',\n",
              "  'и',\n",
              "  'административных',\n",
              "  'условий',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_text_markup = list(_ for _ in n_morph.map(n_sen_chunk))\n",
        "[print_pos(x) for x in n_text_markup]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox7jury8lGBs",
        "outputId": "aa4e04e6-2257-4593-ee0f-4b74a2ae3457"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Повседневная - ADJ|Case=Nom|Degree=Pos|Gender=Fem|Number=Sing\n",
            "практика - NOUN|Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
            "показывает - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            ", - PUNCT\n",
            "что - SCONJ\n",
            "начало - NOUN|Animacy=Inan|Case=Nom|Gender=Neut|Number=Sing\n",
            "повседневной - ADJ|Case=Gen|Degree=Pos|Gender=Fem|Number=Sing\n",
            "работы - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "по - ADP\n",
            "формированию - NOUN|Animacy=Inan|Case=Dat|Gender=Neut|Number=Sing\n",
            "позиции - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "в - ADP\n",
            "значительной - ADJ|Case=Loc|Degree=Pos|Gender=Fem|Number=Sing\n",
            "степени - NOUN|Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing\n",
            "обуславливает - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            "создание - NOUN|Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing\n",
            "модели - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "развития - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "? - PUNCT\n",
            "Таким - DET|Case=Ins|Gender=Masc|Number=Sing\n",
            "образом - NOUN|Animacy=Inan|Case=Ins|Gender=Masc|Number=Sing\n",
            ", - PUNCT\n",
            "курс - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
            "на - ADP\n",
            "социально-ориентированный - ADJ|Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\n",
            "национальный - ADJ|Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\n",
            "проект - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
            "обеспечивает - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            "актуальность - NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing\n",
            "всесторонне - ADV|Degree=Pos\n",
            "сбалансированных - VERB|Aspect=Perf|Case=Gen|Number=Plur|Tense=Past|VerbForm=Part|Voice=Pass\n",
            "нововведений - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
            ". - PUNCT\n",
            "Дорогие - ADJ|Case=Nom|Degree=Pos|Number=Plur\n",
            "друзья - NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Plur\n",
            ", - PUNCT\n",
            "реализация - NOUN|Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
            "намеченного - VERB|Aspect=Perf|Case=Gen|Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass\n",
            "плана - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "развития - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "влечет - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            "за - ADP\n",
            "собой - PRON|Case=Ins\n",
            "процесс - NOUN|Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
            "внедрения - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "и - CCONJ\n",
            "модернизации - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "существующих - VERB|Aspect=Imp|Case=Gen|Number=Plur|Tense=Pres|VerbForm=Part|Voice=Act\n",
            "финансовых - ADJ|Case=Gen|Degree=Pos|Number=Plur\n",
            "и - CCONJ\n",
            "административных - ADJ|Case=Gen|Degree=Pos|Number=Plur\n",
            "условий - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
            ". - PUNCT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_sen_chunk_2 = n_sentenize(text2)"
      ],
      "metadata": {
        "id": "zB2BVHXyJm-f"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_text2_markup = list(n_morph.map(n_sen_chunk_2))\n",
        "[print_pos(x) for x in n_text2_markup]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lR8begRoh2Y",
        "outputId": "3d75471f-d9d5-4278-fc4b-65880bacb324"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Разнообразный - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
            "и - CCONJ\n",
            "богатый - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
            "опыт - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
            "реализация - NOUN|Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
            "намеченного - VERB|Aspect=Perf|Case=Gen|Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass\n",
            "плана - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "развития - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "требует - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            "от - ADP\n",
            "нас - PRON|Case=Gen|Number=Plur|Person=1\n",
            "системного - ADJ|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing\n",
            "анализа - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "дальнейших - ADJ|Case=Gen|Degree=Pos|Number=Plur\n",
            "направлений - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
            "развитая - VERB|Aspect=Perf|Case=Nom|Gender=Fem|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass\n",
            "системы - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "массового - ADJ|Case=Gen|Degree=Pos|Gender=Neut|Number=Sing\n",
            "участия - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            ". - PUNCT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лемматизация"
      ],
      "metadata": {
        "id": "g0okcggK8RyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab"
      ],
      "metadata": {
        "id": "MwcHLMPxlZbv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_lemmatize(text):\n",
        "    emb = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    segmenter = Segmenter()\n",
        "    morph_vocab = MorphVocab()\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "    return doc"
      ],
      "metadata": {
        "id": "QiZyPQqClbSu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc = n_lemmatize(text)\n",
        "{_.text: _.lemma for _ in n_doc.tokens}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_vfR7milc_L",
        "outputId": "18dac6fe-2336-41e5-ec7b-4756d50d33b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': ',',\n",
              " '.': '.',\n",
              " '?': '?',\n",
              " 'Дорогие': 'дорогой',\n",
              " 'Повседневная': 'повседневный',\n",
              " 'Таким': 'такой',\n",
              " 'административных': 'административный',\n",
              " 'актуальность': 'актуальность',\n",
              " 'в': 'в',\n",
              " 'влечет': 'влечь',\n",
              " 'внедрения': 'внедрение',\n",
              " 'всесторонне': 'всесторонне',\n",
              " 'друзья': 'друг',\n",
              " 'за': 'за',\n",
              " 'значительной': 'значительный',\n",
              " 'и': 'и',\n",
              " 'курс': 'курс',\n",
              " 'модели': 'модель',\n",
              " 'модернизации': 'модернизация',\n",
              " 'на': 'на',\n",
              " 'намеченного': 'наметить',\n",
              " 'национальный': 'национальный',\n",
              " 'начало': 'начало',\n",
              " 'нововведений': 'нововведение',\n",
              " 'обеспечивает': 'обеспечивать',\n",
              " 'образом': 'образ',\n",
              " 'обуславливает': 'обуславливать',\n",
              " 'плана': 'план',\n",
              " 'по': 'по',\n",
              " 'повседневной': 'повседневный',\n",
              " 'позиции': 'позиция',\n",
              " 'показывает': 'показывать',\n",
              " 'практика': 'практика',\n",
              " 'проект': 'проект',\n",
              " 'процесс': 'процесс',\n",
              " 'работы': 'работа',\n",
              " 'развития': 'развитие',\n",
              " 'реализация': 'реализация',\n",
              " 'сбалансированных': 'сбалансировать',\n",
              " 'собой': 'себя',\n",
              " 'создание': 'создание',\n",
              " 'социально-ориентированный': 'социально-ориентированный',\n",
              " 'степени': 'степень',\n",
              " 'существующих': 'существовать',\n",
              " 'условий': 'условие',\n",
              " 'финансовых': 'финансовый',\n",
              " 'формированию': 'формирование',\n",
              " 'что': 'что'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc2 = n_lemmatize(text2)\n",
        "{_.text: _.lemma for _ in n_doc2.tokens}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctQlhapPoooX",
        "outputId": "cb7c11bc-225a-4513-9e98-09deb4c83f54"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': '.',\n",
              " 'Разнообразный': 'разнообразный',\n",
              " 'анализа': 'анализ',\n",
              " 'богатый': 'богатый',\n",
              " 'дальнейших': 'дальнейший',\n",
              " 'и': 'и',\n",
              " 'массового': 'массовый',\n",
              " 'намеченного': 'наметить',\n",
              " 'направлений': 'направление',\n",
              " 'нас': 'мы',\n",
              " 'опыт': 'опыт',\n",
              " 'от': 'от',\n",
              " 'плана': 'план',\n",
              " 'развитая': 'развить',\n",
              " 'развития': 'развитие',\n",
              " 'реализация': 'реализация',\n",
              " 'системного': 'системный',\n",
              " 'системы': 'система',\n",
              " 'требует': 'требовать',\n",
              " 'участия': 'участие'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Выделение (распознавание) именованных сущностей"
      ],
      "metadata": {
        "id": "MGiHQ5nV8YHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from slovnet import NER\n",
        "from ipymarkup import show_span_ascii_markup as show_markup"
      ],
      "metadata": {
        "id": "CX1Jn2LjllO4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = NER.load('slovnet_ner_news_v1.tar')"
      ],
      "metadata": {
        "id": "6jR1CB3clmoO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_res = ner.navec(navec)"
      ],
      "metadata": {
        "id": "EfcXKkyLlo_f"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = 'Москва - столица России, по преданию ее основал князь Юрий Долгорукий в 1147 году.'"
      ],
      "metadata": {
        "id": "TLMBZmj1LukS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markup_ner = ner(text3)\n",
        "markup_ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUkgF6nVlrLY",
        "outputId": "07755f00-6fec-4ed5-d75d-220de8950946"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpanMarkup(\n",
              "    text='Москва - столица России, по преданию ее основал князь Юрий Долгорукий в 1147 году.',\n",
              "    spans=[Span(\n",
              "         start=0,\n",
              "         stop=6,\n",
              "         type='LOC'\n",
              "     ), Span(\n",
              "         start=17,\n",
              "         stop=23,\n",
              "         type='LOC'\n",
              "     ), Span(\n",
              "         start=54,\n",
              "         stop=69,\n",
              "         type='PER'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_markup(markup_ner.text, markup_ner.spans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy-71aTplrGm",
        "outputId": "dd640110-e4e0-4db8-8160-6898b5d78578"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Москва - столица России, по преданию ее основал князь Юрий Долгорукий \n",
            "LOC───           LOC───                               PER──────────── \n",
            "в 1147 году.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Разбор предложения"
      ],
      "metadata": {
        "id": "N1EEBFmk8ddx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import NewsSyntaxParser"
      ],
      "metadata": {
        "id": "w07LI_cdnt3A"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = NewsEmbedding()\n",
        "syntax_parser = NewsSyntaxParser(emb)"
      ],
      "metadata": {
        "id": "ad3aEUqunyFr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc.parse_syntax(syntax_parser)\n",
        "n_doc.sents[0].syntax.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbFuGGMinzZu",
        "outputId": "55129f83-56ee-4511-9d8d-182756d6ced6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              ┌► Повседневная  amod\n",
            "            ┌►└─ практика      nsubj\n",
            "┌─┌─────────└─── показывает    \n",
            "│ │ ┌──────────► ,             punct\n",
            "│ │ │ ┌────────► что           mark\n",
            "│ │ │ │ ┌──►┌─── начало        nsubj\n",
            "│ │ │ │ │   │ ┌► повседневной  amod\n",
            "│ │ │ │ │ ┌─└►└─ работы        nmod\n",
            "│ │ │ │ │ │   ┌► по            case\n",
            "│ │ │ │ │ └►┌─└─ формированию  nmod\n",
            "│ │ │ │ │   └──► позиции       nmod\n",
            "│ │ │ │ │   ┌──► в             case\n",
            "│ │ │ │ │   │ ┌► значительной  amod\n",
            "│ │ │ │ │   └─└─ степени       obl\n",
            "│ └►└─└─└───└─┌─ обуславливает ccomp\n",
            "│           ┌─└► создание      obj\n",
            "│           └►┌─ модели        nmod\n",
            "│             └► развития      nmod\n",
            "└──────────────► ?             punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc.parse_syntax(syntax_parser)\n",
        "n_doc.sents[1].syntax.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc4OlSa1n08h",
        "outputId": "579edd7e-0823-48b5-e0fe-f1ae2ecc54d1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ┌► Таким                     det\n",
            "┌──────►┌─└─ образом                   parataxis\n",
            "│       └──► ,                         punct\n",
            "│ ┌►┌─────── курс                      nsubj\n",
            "│ │ │ ┌────► на                        case\n",
            "│ │ │ │ ┌──► социально-ориентированный amod\n",
            "│ │ │ │ │ ┌► национальный              amod\n",
            "│ │ └►└─└─└─ проект                    nmod\n",
            "└─└───────┌─ обеспечивает              \n",
            "│     ┌───└► актуальность              obj\n",
            "│     │   ┌► всесторонне               advmod\n",
            "│     │ ┌►└─ сбалансированных          acl\n",
            "│     └►└─── нововведений              nmod\n",
            "└──────────► .                         punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc.parse_syntax(syntax_parser)\n",
        "n_doc.sents[2].syntax.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7a2nHbUn9-4",
        "outputId": "e2dcc0ef-97b9-49ee-eab2-d1f5b0e239c4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ┌► Дорогие          amod\n",
            "    ┌──►┌─└─ друзья           nsubj\n",
            "    │   │ ┌► ,                punct\n",
            "    │ ┌─└►└─ реализация       conj\n",
            "    │ │   ┌► намеченного      amod\n",
            "    │ └►┌─└─ плана            nmod\n",
            "    │   └──► развития         nmod\n",
            "┌───└─┌─┌─── влечет           \n",
            "│     │ │ ┌► за               case\n",
            "│     │ └►└─ собой            obl\n",
            "│     └──►┌─ процесс          nsubj\n",
            "│ ┌─────┌─└► внедрения        nmod\n",
            "│ │     │ ┌► и                cc\n",
            "│ │     └►└─ модернизации     conj\n",
            "│ │ ┌──────► существующих     amod\n",
            "│ │ │ ┌►┌─── финансовых       amod\n",
            "│ │ │ │ │ ┌► и                cc\n",
            "│ │ │ │ └►└─ административных conj\n",
            "│ └►└─└───── условий          nmod\n",
            "└──────────► .                punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_doc2.parse_syntax(syntax_parser)\n",
        "n_doc2.sents[0].syntax.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QcEdIjQoEX9",
        "outputId": "b4d7bf04-4699-4fcc-af85-8094fdf99853"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ┌────────────► Разнообразный amod\n",
            "  │           ┌► и             cc\n",
            "  │           └─ богатый       \n",
            "┌─│     ┌──────► опыт          nsubj\n",
            "│ │     │ ┌►┌─── реализация    obj\n",
            "│ │     │ │ │ ┌► намеченного   amod\n",
            "│ │     │ │ └►└─ плана         nmod\n",
            "│ │     │ │ └──► развития      nmod\n",
            "│ │ ┌─┌─└─└─┌─── требует       \n",
            "│ │ │ │ │   │ ┌► от            case\n",
            "│ │ │ │ │   └►└─ нас           obl\n",
            "│ │ │ │ │     ┌► системного    amod\n",
            "│ └─│ │ └──►┌─└─ анализа       obj\n",
            "│   │ │     │ ┌► дальнейших    amod\n",
            "│   │ │     └►└─ направлений   nmod\n",
            "└──►│ │          развитая      amod\n",
            "    │ └────►┌─── системы       nsubj\n",
            "    │       │ ┌► массового     amod\n",
            "    │       └►└─ участия       nmod\n",
            "    └──────────► .             punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "sns.set(style=\"ticks\")"
      ],
      "metadata": {
        "id": "-c2XPKKOq7Wu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Векторизация текста на основе модели \"мешка слов\""
      ],
      "metadata": {
        "id": "bAzmgaJ_80sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"rec.autos\", \"rec.sport.hockey\", \"sci.crypt\",\"sci.space\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "metadata": {
        "id": "VLoXLDqzq4Ku"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "metadata": {
        "id": "42Wh-CynrkGj"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabVect = CountVectorizer()\n",
        "vocabVect.fit(data)\n",
        "corpusVocab = vocabVect.vocabulary_\n",
        "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esrSY6qXrleU",
        "outputId": "68d4fb76-4161-4b97-8a12-0db2efd9a200"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество сформированных признаков - 37036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(corpusVocab)[1:10]:\n",
        "    print('{}={}'.format(i, corpusVocab[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UA7wvzZsJxl",
        "outputId": "9bb146b5-ef60-4dd6-ea58-be49bf1f0cc9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jake=19872\n",
            "rambler=27980\n",
            "eng=14514\n",
            "sun=32456\n",
            "com=10748\n",
            "jason=19918\n",
            "cockroft=10613\n",
            "subject=32275\n",
            "re=28120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Использование класса CountVectorizer"
      ],
      "metadata": {
        "id": "p_MD1mfu9BUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = vocabVect.transform(data)\n",
        "test_features"
      ],
      "metadata": {
        "id": "EOcZ0afBs98g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3484af4-c207-42ca-8fa3-52022ce58589"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2382x37036 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 393370 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.todense()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olYUOhsstEZ1",
        "outputId": "eaa2265d-55de-42b3-dcb6-1bfba2b8450f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Размер нулевой строки\n",
        "len(test_features.todense()[0].getA1())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-56OfvNtH9r",
        "outputId": "c8544211-4332-4c13-f373-6e764d3e4b37"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37036"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Непустые значения нулевой строки\n",
        "print([i for i in test_features.todense()[0].getA1() if i>0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQND6-DtKEh",
        "outputId": "1f5acf04-be60-4d5f-bac7-1e7d07615cb8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 2, 1, 1, 1, 1, 7, 1, 2, 1, 1, 1, 1, 1, 1, 8, 2, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 2, 10, 1, 2, 1, 2, 8, 1, 3, 1, 2, 2, 1, 9, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 6, 1, 2, 2, 1, 1, 1, 1, 1, 1, 5, 39, 1, 12, 8, 3, 1, 1, 2, 3, 5, 3, 1, 2, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 5, 1, 2, 1, 1, 1, 1, 3, 1, 4, 1, 6, 3, 9, 1, 1, 1, 1, 1, 1, 1, 1, 2, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 4, 1, 1, 1, 4, 2, 7, 20, 3, 4, 1, 4, 2, 2, 2, 1, 7, 1, 1, 1, 2, 1, 2, 1, 16, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 9, 5, 1, 5, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabVect.get_feature_names()[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZMU_R7GtS6d",
        "outputId": "16f3921e-5711-4389-ca17-22f5b9bae354"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '0000',\n",
              " '00000',\n",
              " '000000',\n",
              " '00000000',\n",
              " '00000000b',\n",
              " '00000001',\n",
              " '00000001b',\n",
              " '00000010']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Решение задачи анализа тональности текста на основе модели \"мешка слов\""
      ],
      "metadata": {
        "id": "JsJo3ST09MHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
        "    for v in vectorizers_list:\n",
        "        for c in classifiers_list:\n",
        "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
        "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
        "            print('Векторизация - {}'.format(v))\n",
        "            print('Модель для классификации - {}'.format(c))\n",
        "            print('Accuracy = {}'.format(score))\n",
        "            print('===========================')"
      ],
      "metadata": {
        "id": "VV9Gdq0At4yY"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab)]\n",
        "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
        "VectorizeAndClassify(vectorizers_list, classifiers_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdDlkVmyt9uu",
        "outputId": "97e0d858-6673-4c3a-b30e-694f6d272773"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '00000000': 5, '00000000b': 6,\n",
            "                            '00000001': 7, '00000001b': 8, '00000010': 9,\n",
            "                            '00000010b': 10, '00000011': 11, '00000011b': 12,\n",
            "                            '00000100': 13, '00000100b': 14, '00000101': 15,\n",
            "                            '00000101b': 16, '00000110': 17, '00000110b': 18,\n",
            "                            '00000111': 19, '00000111b': 20, '00001000': 21,\n",
            "                            '00001000b': 22, '00001001': 23, '00001001b': 24,\n",
            "                            '00001010': 25, '00001010b': 26, '00001011': 27,\n",
            "                            '00001011b': 28, '00001100': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0)\n",
            "Accuracy = 0.971872376154492\n",
            "===========================\n",
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '00000000': 5, '00000000b': 6,\n",
            "                            '00000001': 7, '00000001b': 8, '00000010': 9,\n",
            "                            '00000010b': 10, '00000011': 11, '00000011b': 12,\n",
            "                            '00000100': 13, '00000100b': 14, '00000101': 15,\n",
            "                            '00000101b': 16, '00000110': 17, '00000110b': 18,\n",
            "                            '00000111': 19, '00000111b': 20, '00001000': 21,\n",
            "                            '00001000b': 22, '00001001': 23, '00001001b': 24,\n",
            "                            '00001010': 25, '00001010b': 26, '00001011': 27,\n",
            "                            '00001011b': 28, '00001100': 29, ...})\n",
            "Модель для классификации - LinearSVC()\n",
            "Accuracy = 0.9748110831234257\n",
            "===========================\n",
            "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '00000000': 5, '00000000b': 6,\n",
            "                            '00000001': 7, '00000001b': 8, '00000010': 9,\n",
            "                            '00000010b': 10, '00000011': 11, '00000011b': 12,\n",
            "                            '00000100': 13, '00000100b': 14, '00000101': 15,\n",
            "                            '00000101b': 16, '00000110': 17, '00000110b': 18,\n",
            "                            '00000111': 19, '00000111b': 20, '00001000': 21,\n",
            "                            '00001000b': 22, '00001001': 23, '00001001b': 24,\n",
            "                            '00001010': 25, '00001010b': 26, '00001011': 27,\n",
            "                            '00001011b': 28, '00001100': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier()\n",
            "Accuracy = 0.7065491183879092\n",
            "===========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
      ],
      "metadata": {
        "id": "x2mfQKgl9RN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups['data'], newsgroups['target'], test_size=0.5, random_state=1)"
      ],
      "metadata": {
        "id": "m77McUr1uylj"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "metadata": {
        "id": "7g6e4RKBvIHn"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment(CountVectorizer(), LinearSVC())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcvoZrEgvFRk",
        "outputId": "ae14f9c8-0ed8-4333-d02d-a6df818345ae"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.973421926910299\n",
            "1 \t 0.9765886287625418\n",
            "2 \t 0.9419795221843004\n",
            "3 \t 0.9832214765100671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Работа с векторными представлениями слов с использованием word2vec"
      ],
      "metadata": {
        "id": "M8gze-2y9bRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "v7Y1WfrRwAWR"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
      ],
      "metadata": {
        "id": "TatHWijIwB7r"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "metadata": {
        "id": "yqBiwzzlwE9W"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['жар_S', 'тепло_S', 'вода_S', 'лёд_S']"
      ],
      "metadata": {
        "id": "LgDxS_I6wIMh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    if word in model:\n",
        "        print('\\nСЛОВО - {}'.format(word))\n",
        "        print('5 ближайших соседей слова:')\n",
        "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
        "            print('{} => {}'.format(word, sim))\n",
        "    else:\n",
        "        print('Слово \"{}\" не найдено в модели'.format(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy5v9XQQxHm1",
        "outputId": "3c84cd7d-cbf4-4374-de96-2411f739e7e6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "СЛОВО - жар_S\n",
            "5 ближайших соседей слова:\n",
            "жара_S => 0.5225024819374084\n",
            "зной_S => 0.5192716717720032\n",
            "озноб_S => 0.508800745010376\n",
            "пламень_S => 0.49908268451690674\n",
            "холод_S => 0.49906834959983826\n",
            "\n",
            "СЛОВО - тепло_S\n",
            "5 ближайших соседей слова:\n",
            "теплота_S => 0.6763811111450195\n",
            "теплый_A => 0.6017489433288574\n",
            "тепло_ADV => 0.5947409868240356\n",
            "прохлада_S => 0.5492426156997681\n",
            "согревать_V => 0.494480699300766\n",
            "\n",
            "СЛОВО - вода_S\n",
            "5 ближайших соседей слова:\n",
            "вод_S => 0.6678440570831299\n",
            "водичка_S => 0.639014482498169\n",
            "влага_S => 0.5951642990112305\n",
            "водица_S => 0.5687029361724854\n",
            "струя_S => 0.5454239249229431\n",
            "Слово \"лёд_S\" не найдено в модели\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Находим близость между словами и строим аналогии"
      ],
      "metadata": {
        "id": "n0RDfHMr9sgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.similarity('жар_S', 'тепло_S'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cTsFYXWxL9K",
        "outputId": "71eccde6-ce56-40cf-dd5b-511a462a13fd"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45076987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar(positive=['жар_S', 'теплота_S'], negative=['тепло_S']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI2D3nmtxMao",
        "outputId": "4e5df264-7d38-4a22-ce35-9ff5cc901255"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('горячность_S', 0.4428492784500122), ('страстность_S', 0.43567872047424316), ('ожесточение_S', 0.43431538343429565), ('нежность_S', 0.41775262355804443), ('задушевность_S', 0.4013420343399048), ('волнение_S', 0.398081511259079), ('пафос_S', 0.39539504051208496), ('пронзительность_S', 0.3931868076324463), ('искренность_S', 0.3875563144683838), ('озноб_S', 0.3868481516838074)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Обучим word2vec на наборе данных \"fetch_20newsgroups\""
      ],
      "metadata": {
        "id": "l3dZXqKE9zrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIABoRcH5tWG",
        "outputId": "15e1ba53-6553-47d3-a799-c8eddaa1d59f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"rec.autos\", \"rec.sport.hockey\", \"sci.crypt\",\"sci.space\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "metadata": {
        "id": "jZbwRKof5zb9"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовим корпус\n",
        "corpus = []\n",
        "stop_words = stopwords.words('english')\n",
        "tok = WordPunctTokenizer()\n",
        "for line in newsgroups['data']:\n",
        "    line1 = line.strip().lower()\n",
        "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
        "    text_tok = tok.tokenize(line1)\n",
        "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
        "    corpus.append(text_tok1)"
      ],
      "metadata": {
        "id": "qXFAWpOR6akB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8AkEoW6vg4",
        "outputId": "f75c9aac-1e82-4768-da16-49aff242312b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['jake',\n",
              "  'rambler',\n",
              "  'eng',\n",
              "  'sun',\n",
              "  'com',\n",
              "  'jason',\n",
              "  'cockroft',\n",
              "  'subject',\n",
              "  'playoff',\n",
              "  'predictions',\n",
              "  'organization',\n",
              "  'sun',\n",
              "  'microsystems',\n",
              "  'inc',\n",
              "  'lines',\n",
              "  'distribution',\n",
              "  'world',\n",
              "  'reply',\n",
              "  'jake',\n",
              "  'rambler',\n",
              "  'eng',\n",
              "  'sun',\n",
              "  'com',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'rambler',\n",
              "  'eng',\n",
              "  'sun',\n",
              "  'com',\n",
              "  'st',\n",
              "  'round',\n",
              "  'pitt',\n",
              "  'vs',\n",
              "  'nyi',\n",
              "  'pitt',\n",
              "  'looks',\n",
              "  'like',\n",
              "  'safe',\n",
              "  'bet',\n",
              "  'nyi',\n",
              "  'bagging',\n",
              "  'late',\n",
              "  'nyi',\n",
              "  'njd',\n",
              "  'showdown',\n",
              "  'friday',\n",
              "  'night',\n",
              "  'honour',\n",
              "  'pittsburg',\n",
              "  'anyway',\n",
              "  'pigsburg',\n",
              "  'wash',\n",
              "  'vs',\n",
              "  'njd',\n",
              "  'wash',\n",
              "  'think',\n",
              "  'njd',\n",
              "  'solid',\n",
              "  'team',\n",
              "  'compete',\n",
              "  'wash',\n",
              "  'agree',\n",
              "  'though',\n",
              "  'wash',\n",
              "  'bos',\n",
              "  'vs',\n",
              "  'buf',\n",
              "  'bos',\n",
              "  'b',\n",
              "  'playing',\n",
              "  'awesome',\n",
              "  'hockey',\n",
              "  'last',\n",
              "  'two',\n",
              "  'weeks',\n",
              "  'question',\n",
              "  'long',\n",
              "  'last',\n",
              "  'fuhr',\n",
              "  'dud',\n",
              "  'bos',\n",
              "  'que',\n",
              "  'vs',\n",
              "  'mon',\n",
              "  'mon',\n",
              "  'seems',\n",
              "  'mon',\n",
              "  'much',\n",
              "  'like',\n",
              "  'van',\n",
              "  'chemistry',\n",
              "  'habs',\n",
              "  'seem',\n",
              "  'stride',\n",
              "  'que',\n",
              "  'chi',\n",
              "  'vs',\n",
              "  'stl',\n",
              "  'chi',\n",
              "  'stl',\n",
              "  'playoffs',\n",
              "  'chi',\n",
              "  'det',\n",
              "  'vs',\n",
              "  'tor',\n",
              "  'det',\n",
              "  'diehard',\n",
              "  'leaf',\n",
              "  'fan',\n",
              "  'seems',\n",
              "  'leafs',\n",
              "  'offense',\n",
              "  'shutting',\n",
              "  'last',\n",
              "  'week',\n",
              "  'turn',\n",
              "  'around',\n",
              "  'detroit',\n",
              "  'recall',\n",
              "  'last',\n",
              "  'couple',\n",
              "  'time',\n",
              "  'two',\n",
              "  'teams',\n",
              "  'met',\n",
              "  'leafs',\n",
              "  'pummelled',\n",
              "  'know',\n",
              "  'bobbie',\n",
              "  'allowed',\n",
              "  'canada',\n",
              "  'yet',\n",
              "  'det',\n",
              "  'det',\n",
              "  'van',\n",
              "  'vs',\n",
              "  'win',\n",
              "  'win',\n",
              "  'upset',\n",
              "  'making',\n",
              "  'another',\n",
              "  'team',\n",
              "  'bad',\n",
              "  'chemistry',\n",
              "  'something',\n",
              "  'gone',\n",
              "  'foul',\n",
              "  'among',\n",
              "  'linden',\n",
              "  'momesso',\n",
              "  'bure',\n",
              "  'win',\n",
              "  'cal',\n",
              "  'vs',\n",
              "  'la',\n",
              "  'cal',\n",
              "  'anybody',\n",
              "  'says',\n",
              "  'la',\n",
              "  'could',\n",
              "  'possibly',\n",
              "  'beet',\n",
              "  'cal',\n",
              "  'watch',\n",
              "  'smythe',\n",
              "  'whole',\n",
              "  'lot',\n",
              "  'la',\n",
              "  'bunch',\n",
              "  'geritols',\n",
              "  'cal',\n",
              "  'nd',\n",
              "  'round',\n",
              "  'pitt',\n",
              "  'vs',\n",
              "  'wash',\n",
              "  'pitt',\n",
              "  'seems',\n",
              "  'pigsburg',\n",
              "  'egos',\n",
              "  'team',\n",
              "  'saving',\n",
              "  'grace',\n",
              "  'though',\n",
              "  'bowman',\n",
              "  'put',\n",
              "  'anybody',\n",
              "  'place',\n",
              "  'however',\n",
              "  'pigs',\n",
              "  'quick',\n",
              "  'first',\n",
              "  'round',\n",
              "  'may',\n",
              "  'little',\n",
              "  'high',\n",
              "  'wash',\n",
              "  'could',\n",
              "  'surprise',\n",
              "  'said',\n",
              "  'say',\n",
              "  'pitt',\n",
              "  'bos',\n",
              "  'vs',\n",
              "  'mon',\n",
              "  'bos',\n",
              "  'mon',\n",
              "  'bos',\n",
              "  'surprising',\n",
              "  'late',\n",
              "  'cam',\n",
              "  'great',\n",
              "  'couple',\n",
              "  'wins',\n",
              "  'que',\n",
              "  'last',\n",
              "  'week',\n",
              "  'sold',\n",
              "  'b',\n",
              "  'b',\n",
              "  'chi',\n",
              "  'vs',\n",
              "  'det',\n",
              "  'chi',\n",
              "  'yikes',\n",
              "  'pretty',\n",
              "  'det',\n",
              "  'running',\n",
              "  'like',\n",
              "  'machine',\n",
              "  'late',\n",
              "  'non',\n",
              "  'busy',\n",
              "  'end',\n",
              "  'season',\n",
              "  'played',\n",
              "  'like',\n",
              "  'killers',\n",
              "  'det',\n",
              "  'win',\n",
              "  'vs',\n",
              "  'cal',\n",
              "  'cal',\n",
              "  'cal',\n",
              "  'solid',\n",
              "  'team',\n",
              "  'little',\n",
              "  'weak',\n",
              "  'nets',\n",
              "  'cal',\n",
              "  'muscle',\n",
              "  'win',\n",
              "  'cal',\n",
              "  'rd',\n",
              "  'round',\n",
              "  'pitt',\n",
              "  'vs',\n",
              "  'bos',\n",
              "  'pitt',\n",
              "  'hate',\n",
              "  'pitt',\n",
              "  'logic',\n",
              "  'eludes',\n",
              "  'dark',\n",
              "  'side',\n",
              "  'take',\n",
              "  'give',\n",
              "  'bos',\n",
              "  'extra',\n",
              "  'push',\n",
              "  'needs',\n",
              "  'dump',\n",
              "  'pitt',\n",
              "  'may',\n",
              "  'something',\n",
              "  'think',\n",
              "  'rivalry',\n",
              "  'bos',\n",
              "  'chi',\n",
              "  'vs',\n",
              "  'cal',\n",
              "  'chi',\n",
              "  'finals',\n",
              "  'pitt',\n",
              "  'vs',\n",
              "  'chi',\n",
              "  'pitt',\n",
              "  'bos',\n",
              "  'vs',\n",
              "  'det',\n",
              "  'know',\n",
              "  'say',\n",
              "  'teams',\n",
              "  'flying',\n",
              "  'overdue',\n",
              "  'go',\n",
              "  'goaltending',\n",
              "  'muscle',\n",
              "  'say',\n",
              "  'det',\n",
              "  'jake'],\n",
              " ['dreier',\n",
              "  'jaffna',\n",
              "  'berkeley',\n",
              "  'edu',\n",
              "  'roland',\n",
              "  'dreier',\n",
              "  'subject',\n",
              "  'plus',\n",
              "  'minus',\n",
              "  'stat',\n",
              "  'organization',\n",
              "  'u',\n",
              "  'c',\n",
              "  'berkeley',\n",
              "  'math',\n",
              "  'department',\n",
              "  'lines',\n",
              "  'qmtd',\n",
              "  'innr',\n",
              "  'l',\n",
              "  'iskut',\n",
              "  'ucs',\n",
              "  'ubc',\n",
              "  'ca',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'jaffna',\n",
              "  'berkeley',\n",
              "  'edu',\n",
              "  'reply',\n",
              "  'gibson',\n",
              "  'nukta',\n",
              "  'geop',\n",
              "  'ubc',\n",
              "  'ca',\n",
              "  'message',\n",
              "  'apr',\n",
              "  'gmt',\n",
              "  'article',\n",
              "  'qmtd',\n",
              "  'innr',\n",
              "  'l',\n",
              "  'iskut',\n",
              "  'ucs',\n",
              "  'ubc',\n",
              "  'ca',\n",
              "  'gibson',\n",
              "  'nukta',\n",
              "  'geop',\n",
              "  'ubc',\n",
              "  'ca',\n",
              "  'brad',\n",
              "  'gibson',\n",
              "  'writes',\n",
              "  'article',\n",
              "  'apr',\n",
              "  'sol',\n",
              "  'uvic',\n",
              "  'ca',\n",
              "  'gballent',\n",
              "  'hudson',\n",
              "  'uvic',\n",
              "  'ca',\n",
              "  'writes',\n",
              "  'article',\n",
              "  'blue',\n",
              "  'cis',\n",
              "  'pitt',\n",
              "  'edu',\n",
              "  'jrmst',\n",
              "  'pitt',\n",
              "  'edu',\n",
              "  'joseph',\n",
              "  'r',\n",
              "  'mcdonald',\n",
              "  'writes',\n",
              "  'jagr',\n",
              "  'higher',\n",
              "  'francis',\n",
              "  'points',\n",
              "  'take',\n",
              "  'informed',\n",
              "  'observer',\n",
              "  'ronnie',\n",
              "  'francis',\n",
              "  'much',\n",
              "  'better',\n",
              "  'season',\n",
              "  'jaromir',\n",
              "  'jagr',\n",
              "  'take',\n",
              "  'anything',\n",
              "  'away',\n",
              "  'jaro',\n",
              "  'decent',\n",
              "  'year',\n",
              "  'although',\n",
              "  'live',\n",
              "  'expectations',\n",
              "  'bowman',\n",
              "  'tended',\n",
              "  'overplay',\n",
              "  'francis',\n",
              "  'times',\n",
              "  'bowman',\n",
              "  'style',\n",
              "  'player',\n",
              "  'plays',\n",
              "  'hard',\n",
              "  'times',\n",
              "  'disregard',\n",
              "  'defensive',\n",
              "  'responsibilities',\n",
              "  'good',\n",
              "  'leader',\n",
              "  'bowman',\n",
              "  'rewarded',\n",
              "  'increasing',\n",
              "  'ice',\n",
              "  'time',\n",
              "  'jagr',\n",
              "  'arrogant',\n",
              "  'juvenile',\n",
              "  'display',\n",
              "  'first',\n",
              "  'attitude',\n",
              "  'rubbed',\n",
              "  'bowman',\n",
              "  'wrong',\n",
              "  'way',\n",
              "  'caused',\n",
              "  'lose',\n",
              "  'ice',\n",
              "  'time',\n",
              "  'throughout',\n",
              "  'year',\n",
              "  'francis',\n",
              "  'consistently',\n",
              "  'recieved',\n",
              "  'ice',\n",
              "  'time',\n",
              "  'jagr',\n",
              "  'althouhg',\n",
              "  'never',\n",
              "  'seen',\n",
              "  'stats',\n",
              "  'subject',\n",
              "  'pretty',\n",
              "  'sure',\n",
              "  'jagr',\n",
              "  'points',\n",
              "  'per',\n",
              "  'minute',\n",
              "  'played',\n",
              "  'francis',\n",
              "  'add',\n",
              "  'jagr',\n",
              "  'better',\n",
              "  'rating',\n",
              "  'think',\n",
              "  'becomes',\n",
              "  'evident',\n",
              "  'jagr',\n",
              "  'better',\n",
              "  'season',\n",
              "  'francis',\n",
              "  'bad',\n",
              "  'one',\n",
              "  'actually',\n",
              "  'think',\n",
              "  'become',\n",
              "  'evident',\n",
              "  'determined',\n",
              "  'flaunt',\n",
              "  'ignorance',\n",
              "  'cost',\n",
              "  'jagr',\n",
              "  'better',\n",
              "  'season',\n",
              "  'francis',\n",
              "  'suggest',\n",
              "  'otherwise',\n",
              "  'insult',\n",
              "  'modicum',\n",
              "  'hockey',\n",
              "  'knowledge',\n",
              "  'save',\n",
              "  'almost',\n",
              "  'maniacal',\n",
              "  'devotion',\n",
              "  'almighty',\n",
              "  'plus',\n",
              "  'minus',\n",
              "  'misleading',\n",
              "  'hockey',\n",
              "  'stat',\n",
              "  'available',\n",
              "  'nhl',\n",
              "  'publishes',\n",
              "  'useful',\n",
              "  'quantifiable',\n",
              "  'statistic',\n",
              "  'including',\n",
              "  'ice',\n",
              "  'time',\n",
              "  'per',\n",
              "  'game',\n",
              "  'measure',\n",
              "  'quality',\n",
              "  'e',\n",
              "  'player',\n",
              "  'put',\n",
              "  'key',\n",
              "  'situations',\n",
              "  'like',\n",
              "  'protecting',\n",
              "  'lead',\n",
              "  'late',\n",
              "  'game',\n",
              "  'matched',\n",
              "  'team',\n",
              "  'top',\n",
              "  'one',\n",
              "  'two',\n",
              "  'lines',\n",
              "  'short',\n",
              "  'handed',\n",
              "  'etc',\n",
              "  'would',\n",
              "  'much',\n",
              "  'rather',\n",
              "  'see',\n",
              "  'disappear',\n",
              "  'altogether',\n",
              "  'instead',\n",
              "  'dubious',\n",
              "  'merits',\n",
              "  'trumpeted',\n",
              "  'little',\n",
              "  'understanding',\n",
              "  'implications',\n",
              "  'thank',\n",
              "  'posting',\n",
              "  'person',\n",
              "  'first',\n",
              "  'brought',\n",
              "  'fact',\n",
              "  'jagr',\n",
              "  'much',\n",
              "  'higher',\n",
              "  'francis',\n",
              "  'assure',\n",
              "  'brought',\n",
              "  'example',\n",
              "  'absurdity',\n",
              "  'comparisons',\n",
              "  'even',\n",
              "  'team',\n",
              "  'never',\n",
              "  'ever',\n",
              "  'thought',\n",
              "  'anyone',\n",
              "  'would',\n",
              "  'argue',\n",
              "  'jagr',\n",
              "  'higher',\n",
              "  'actually',\n",
              "  'reflected',\n",
              "  'better',\n",
              "  'two',\n",
              "  'way',\n",
              "  'play',\n",
              "  'opinion',\n",
              "  'francis',\n",
              "  'low',\n",
              "  'purely',\n",
              "  'result',\n",
              "  'asked',\n",
              "  'play',\n",
              "  'opponents',\n",
              "  'top',\n",
              "  'scorers',\n",
              "  'times',\n",
              "  'fact',\n",
              "  'chip',\n",
              "  'points',\n",
              "  'neutralizing',\n",
              "  'team',\n",
              "  'top',\n",
              "  'center',\n",
              "  'testament',\n",
              "  'valuable',\n",
              "  'even',\n",
              "  'suffers',\n",
              "  'hand',\n",
              "  'jagr',\n",
              "  'big',\n",
              "  'fast',\n",
              "  'skilled',\n",
              "  'even',\n",
              "  'get',\n",
              "  'points',\n",
              "  'matter',\n",
              "  'inflated',\n",
              "  'way',\n",
              "  'get',\n",
              "  'wrong',\n",
              "  'like',\n",
              "  'jagr',\n",
              "  'may',\n",
              "  'lazy',\n",
              "  'floater',\n",
              "  'turns',\n",
              "  'exactly',\n",
              "  'right',\n",
              "  'times',\n",
              "  'like',\n",
              "  'overtime',\n",
              "  'playoff',\n",
              "  'games',\n",
              "  'roland',\n",
              "  'dreier',\n",
              "  'dreier',\n",
              "  'math',\n",
              "  'berkeley',\n",
              "  'edu'],\n",
              " ['pat',\n",
              "  'rwing',\n",
              "  'uucp',\n",
              "  'pat',\n",
              "  'myrto',\n",
              "  'subject',\n",
              "  'tapped',\n",
              "  'code',\n",
              "  'good',\n",
              "  'article',\n",
              "  'rwing',\n",
              "  'distribution',\n",
              "  'na',\n",
              "  'organization',\n",
              "  'totally',\n",
              "  'unorganized',\n",
              "  'lines',\n",
              "  'article',\n",
              "  'r',\n",
              "  'mc',\n",
              "  'access',\n",
              "  'digex',\n",
              "  'net',\n",
              "  'steve',\n",
              "  'b',\n",
              "  'access',\n",
              "  'digex',\n",
              "  'com',\n",
              "  'steve',\n",
              "  'brinich',\n",
              "  'writes',\n",
              "  'wonder',\n",
              "  'landed',\n",
              "  'fat',\n",
              "  'fee',\n",
              "  'cooperation',\n",
              "  'nsa',\n",
              "  'design',\n",
              "  'propoganda',\n",
              "  'stages',\n",
              "  'care',\n",
              "  'say',\n",
              "  'nsa',\n",
              "  'totally',\n",
              "  'perfidious',\n",
              "  'least',\n",
              "  'redeeming',\n",
              "  'virtue',\n",
              "  'taking',\n",
              "  'care',\n",
              "  'g',\n",
              "  'course',\n",
              "  'take',\n",
              "  'care',\n",
              "  'well',\n",
              "  'person',\n",
              "  'outlived',\n",
              "  'undefined',\n",
              "  'usefulness',\n",
              "  'elimination',\n",
              "  'becomes',\n",
              "  'consideration',\n",
              "  'pat',\n",
              "  'rwing',\n",
              "  'uucp',\n",
              "  'without',\n",
              "  'prejudice',\n",
              "  'ucc',\n",
              "  'pat',\n",
              "  'myrto',\n",
              "  'seattle',\n",
              "  'wa',\n",
              "  'else',\n",
              "  'fails',\n",
              "  'try',\n",
              "  'uunet',\n",
              "  'pilchuck',\n",
              "  'rwing',\n",
              "  'pat',\n",
              "  'wisdom',\n",
              "  'two',\n",
              "  'things',\n",
              "  'infinite',\n",
              "  'universe',\n",
              "  'human',\n",
              "  'stupidity',\n",
              "  'sure',\n",
              "  'former',\n",
              "  'albert',\n",
              "  'einstien'],\n",
              " ['shadow',\n",
              "  'r',\n",
              "  'node',\n",
              "  'hub',\n",
              "  'org',\n",
              "  'jay',\n",
              "  'chu',\n",
              "  'subject',\n",
              "  'lindros',\n",
              "  'traded',\n",
              "  'organization',\n",
              "  'lindros',\n",
              "  'traded',\n",
              "  'summary',\n",
              "  'babe',\n",
              "  'lindros',\n",
              "  'going',\n",
              "  'ottawa',\n",
              "  'lines',\n",
              "  'true',\n",
              "  'rumor',\n",
              "  'fact',\n",
              "  'big',\n",
              "  'three',\n",
              "  'way',\n",
              "  'deal',\n",
              "  'eric',\n",
              "  'lindros',\n",
              "  'going',\n",
              "  'ottawa',\n",
              "  'senators',\n",
              "  'senators',\n",
              "  'get',\n",
              "  'mill',\n",
              "  'montreal',\n",
              "  'montreal',\n",
              "  'gets',\n",
              "  'alexander',\n",
              "  'daigle',\n",
              "  'first',\n",
              "  'round',\n",
              "  'pick',\n",
              "  'senators',\n",
              "  'philly',\n",
              "  'gets',\n",
              "  'damphousse',\n",
              "  'bellow',\n",
              "  'patrick',\n",
              "  'roy',\n",
              "  'draft',\n",
              "  'pick',\n",
              "  'shadow',\n",
              "  'r',\n",
              "  'node',\n",
              "  'gts',\n",
              "  'org',\n",
              "  'see',\n",
              "  'real',\n",
              "  'see',\n",
              "  'transparent',\n",
              "  'see',\n",
              "  'erased'],\n",
              " ['rins',\n",
              "  'ryukoku',\n",
              "  'ac',\n",
              "  'jp',\n",
              "  'william',\n",
              "  'reiken',\n",
              "  'subject',\n",
              "  'nuclear',\n",
              "  'waste',\n",
              "  'organization',\n",
              "  'ryukoku',\n",
              "  'univ',\n",
              "  'seta',\n",
              "  'japan',\n",
              "  'lines',\n",
              "  'article',\n",
              "  'pp',\n",
              "  'reinnonl',\n",
              "  'phantom',\n",
              "  'gatech',\n",
              "  'edu',\n",
              "  'matthew',\n",
              "  'phantom',\n",
              "  'gatech',\n",
              "  'edu',\n",
              "  'matthew',\n",
              "  'deluca',\n",
              "  'writes',\n",
              "  'greedy',\n",
              "  'little',\n",
              "  'oil',\n",
              "  'companies',\n",
              "  'blame',\n",
              "  'oil',\n",
              "  'companies',\n",
              "  'supply',\n",
              "  'demand',\n",
              "  'created',\n",
              "  'everyone',\n",
              "  'else',\n",
              "  'planet',\n",
              "  'run',\n",
              "  'faults',\n",
              "  'ok',\n",
              "  'creation',\n",
              "  'oil',\n",
              "  'producing',\n",
              "  'bacteria',\n",
              "  'figure',\n",
              "  'make',\n",
              "  'eat',\n",
              "  'make',\n",
              "  'shit',\n",
              "  'comments']]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLEebm1K7L41",
        "outputId": "1f740ca6-f87d-40db-812c-21f0037a2258"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.92 s, sys: 33.2 ms, total: 8.95 s\n",
            "Wall time: 5.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, что модель обучилась\n",
        "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klyXfeAR7SeB",
        "outputId": "6a0807e1-0262-43f5-93b2-22799dcd9152"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reverse', 0.9620099067687988), ('could', 0.9579818844795227), ('easy', 0.9559280872344971), ('anything', 0.9553707838058472), ('change', 0.9521973133087158)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_2(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "metadata": {
        "id": "LjM2Ehr-7W0K"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Проверка качества работы модели word2vec"
      ],
      "metadata": {
        "id": "9yUJ1Ynz_2DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingVectorizer(object):\n",
        "    '''\n",
        "    Для текста усредним вектора входящих в него слов\n",
        "    '''\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean(\n",
        "            [self.model[w] for w in words if w in self.model] \n",
        "            or [np.zeros(self.size)], axis=0)\n",
        "            for words in X])"
      ],
      "metadata": {
        "id": "cL1m4h3M7eMo"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "metadata": {
        "id": "iNEtVYom7iCJ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучающая и тестовая выборки\n",
        "boundary = 1500\n",
        "X_train = corpus[:boundary] \n",
        "X_test = corpus[boundary:]\n",
        "y_train = newsgroups['target'][:boundary]\n",
        "y_test = newsgroups['target'][boundary:]"
      ],
      "metadata": {
        "id": "BLZ9TMCd7ksg"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_2(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvQx4dv73ls",
        "outputId": "84ac8c95-3b29-421a-fd59-f9e10fbc5a28"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.9279279279279279\n",
            "1 \t 0.9389671361502347\n",
            "2 \t 0.9417040358744395\n",
            "3 \t 0.9241071428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Как видно из результатов проверки качества моделей, лучшее качество показала модель на основе CountVectorizer*"
      ],
      "metadata": {
        "id": "GnUKW0uhFBJa"
      }
    }
  ]
}